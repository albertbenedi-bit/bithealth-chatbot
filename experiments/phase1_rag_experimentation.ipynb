{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8cee666",
   "metadata": {},
   "source": [
    "##Install necessary libraries in your Python environment where you run Jupyter:\n",
    "Bash\n",
    "pip install jupyterlab langchain sqlalchemy psycopg2-binary pgvector sentence-transformers pypdf python-dotenv google-generativeai # Add other loaders like docx if needed\n",
    "\n",
    "#jupyter notebook --no-browser --ip=127.0.0.1 --port=8888\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a943f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/tan.joenarto/.local/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: langchain_google_genai in /home/tan.joenarto/.local/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: langchain_community in /home/tan.joenarto/.local/lib/python3.12/site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain in /home/tan.joenarto/.local/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: sqlalchemy in /home/tan.joenarto/.local/lib/python3.12/site-packages (2.0.23)\n",
      "Requirement already satisfied: psycopg2-binary in /home/tan.joenarto/.local/lib/python3.12/site-packages (2.9.9)\n",
      "Requirement already satisfied: pgvector in /home/tan.joenarto/.local/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: sentence-transformers in /home/tan.joenarto/.local/lib/python3.12/site-packages (5.0.0)\n",
      "Requirement already satisfied: pypdf in /home/tan.joenarto/.local/lib/python3.12/site-packages (5.5.0)\n",
      "Requirement already satisfied: python-dotenv in /home/tan.joenarto/.local/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: google-generativeai in /home/tan.joenarto/.local/lib/python3.12/site-packages (0.8.5)\n",
      "Requirement already satisfied: filelock in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.12/site-packages (from torch) (68.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (2025.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_google_genai) (1.2.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_google_genai) (0.3.60)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_google_genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (0.3.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain_community) (2.2.6)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sqlalchemy) (3.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (4.52.1)\n",
      "Requirement already satisfied: tqdm in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (0.33.4)\n",
      "Requirement already satisfied: Pillow in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-generativeai) (2.40.1)\n",
      "Requirement already satisfied: protobuf in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-generativeai) (4.25.8)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.25.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: anyio in /home/tan.joenarto/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tan.joenarto/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: sniffio in /home/tan.joenarto/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/tan.joenarto/.local/lib/python3.12/site-packages (5.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/tan.joenarto/.local/lib/python3.12/site-packages (0.33.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (4.52.1)\n",
      "Requirement already satisfied: tqdm in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface_hub) (2025.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (68.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "# Cell 1: Install Libraries (if not already installed in your env)\n",
    "%pip install torch langchain_google_genai langchain_community langchain sqlalchemy psycopg2-binary pgvector sentence-transformers pypdf python-dotenv google-generativeai\n",
    "%pip install --upgrade sentence-transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Replace with your actual K3s database details (get from Step 5 and Step 6)\n",
    "DB_USER = os.getenv(\"DB_USER\", \"postgres\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"BTxg8LN5lH\") # CHANGE THIS!\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\") # Use localhost because of port-forwarding\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"rag_db\")\n",
    "\n",
    "# AI Studio API Key (if you want to experiment with Google Embeddings/LLM)\n",
    "# Get your API key from https://aistudio.google.com/app/apikey\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Your chosen embedding model for Sentence Transformers\n",
    "EMBEDDING_MODEL_NAME = os.getenv(\"EMBEDDING_MODEL_NAME\", \"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define the connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "print(\"Environment variables loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fe3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0.0\n",
      "Ensuring table 'document_chunks' exists...\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3292\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3271\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3272\u001b[39m \n\u001b[32m   3273\u001b[39m \u001b[33;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3290\u001b[39m \n\u001b[32m   3291\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:452\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m \u001b[33;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \n\u001b[32m    451\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:1269\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m     fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:716\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:169\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:167\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:393\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:678\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:902\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:898\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py:637\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    635\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py:616\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs, **cparams):\n\u001b[32m    615\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Create table if it doesn't exist\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnsuring table \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDocumentChunk.__tablename__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m exists...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mBase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTable check complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Initialize Embedding Model (Sentence Transformers)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:5828\u001b[39m, in \u001b[36mMetaData.create_all\u001b[39m\u001b[34m(self, bind, tables, checkfirst)\u001b[39m\n\u001b[32m   5804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_all\u001b[39m(\n\u001b[32m   5805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5806\u001b[39m     bind: _CreateDropBind,\n\u001b[32m   5807\u001b[39m     tables: Optional[_typing_Sequence[Table]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5808\u001b[39m     checkfirst: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   5809\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5810\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create all tables stored in this metadata.\u001b[39;00m\n\u001b[32m   5811\u001b[39m \n\u001b[32m   5812\u001b[39m \u001b[33;03m    Conditional by default, will not attempt to recreate tables already\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5826\u001b[39m \n\u001b[32m   5827\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5828\u001b[39m     \u001b[43mbind\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_ddl_visitor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5829\u001b[39m \u001b[43m        \u001b[49m\u001b[43mddl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSchemaGenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtables\u001b[49m\n\u001b[32m   5830\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3242\u001b[39m, in \u001b[36mEngine._run_ddl_visitor\u001b[39m\u001b[34m(self, visitorcallable, element, **kwargs)\u001b[39m\n\u001b[32m   3236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_ddl_visitor\u001b[39m(\n\u001b[32m   3237\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3238\u001b[39m     visitorcallable: Type[Union[SchemaGenerator, SchemaDropper]],\n\u001b[32m   3239\u001b[39m     element: SchemaItem,\n\u001b[32m   3240\u001b[39m     **kwargs: Any,\n\u001b[32m   3241\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3242\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_ddl_visitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisitorcallable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3232\u001b[39m, in \u001b[36mEngine.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3205\u001b[39m \u001b[38;5;129m@contextlib\u001b[39m.contextmanager\n\u001b[32m   3206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbegin\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Connection]:\n\u001b[32m   3207\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a context manager delivering a :class:`_engine.Connection`\u001b[39;00m\n\u001b[32m   3208\u001b[39m \u001b[33;03m    with a :class:`.Transaction` established.\u001b[39;00m\n\u001b[32m   3209\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3230\u001b[39m \n\u001b[32m   3231\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3232\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m   3233\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m conn.begin():\n\u001b[32m   3234\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m conn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3268\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3246\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3247\u001b[39m \n\u001b[32m   3248\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3265\u001b[39m \n\u001b[32m   3266\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:147\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = engine.raw_connection()\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2430\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception_noconnection\u001b[39m\u001b[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[39m\n\u001b[32m   2428\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2429\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2432\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    147\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    148\u001b[39m             err, dialect, engine\n\u001b[32m    149\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py:3292\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3271\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3272\u001b[39m \n\u001b[32m   3273\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3290\u001b[39m \n\u001b[32m   3291\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:452\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    445\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \n\u001b[32m    451\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:1269\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1266\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1267\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1272\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:716\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    714\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    719\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:169\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py:167\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:393\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    391\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:678\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:902\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    906\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py:898\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    897\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    900\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py:637\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    634\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    635\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py:616\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs, **cparams):\n\u001b[32m    615\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: (psycopg2.OperationalError) connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "# Cell 3: Initialize Database Connection and Vector Store\n",
    "import sentence_transformers\n",
    "print(sentence_transformers.__version__)\n",
    "from sqlalchemy import create_engine, text\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy import Column, Text, Integer\n",
    "\n",
    "# Langchain components\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings # Uncomment to use Google Embeddings\n",
    "\n",
    "# Define table name for your vectors\n",
    "COLLECTION_NAME = \"internal_knowledge\"\n",
    "\n",
    "# Define base for SQLAlchemy models\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define a simple model for your chunks\n",
    "class DocumentChunk(Base):\n",
    "    __tablename__ = \"document_chunks\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    content = Column(Text)\n",
    "    embedding = Column(Vector(384)) # Adjust dimension based on your embedding model\n",
    "    source = Column(Text) # Add metadata like source file\n",
    "\n",
    "# Create engine and session maker\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "print(f\"Ensuring table '{DocumentChunk.__tablename__}' exists...\")\n",
    "Base.metadata.create_all(engine)\n",
    "print(\"Table check complete.\")\n",
    "\n",
    "# Initialize Embedding Model (Sentence Transformers)\n",
    "try:\n",
    "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "    print(\"Sentence Transformers embedding model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Sentence Transformers model: {e}\")\n",
    "    # Fallback or alternative embedding method can be added here\n",
    "\n",
    "# # Optional: Initialize Google Generative AI Embeddings\n",
    "# if GOOGLE_API_KEY:\n",
    "#     try:\n",
    "#         print(\"Initializing Google Generative AI Embeddings...\")\n",
    "#         google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "#         print(\"Google Generative AI Embeddings initialized.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error initializing Google Embeddings: {e}\")\n",
    "#         google_embeddings = None\n",
    "# else:\n",
    "#     google_embeddings = None\n",
    "#     print(\"GOOGLE_API_KEY not set. Skipping Google Embeddings.\")\n",
    "\n",
    "\n",
    "# Initialize PGVector store (Langchain wrapper)\n",
    "# We'll initialize this later once chunks and embeddings are ready to be added or queried\n",
    "vector_store = None\n",
    "\n",
    "print(\"Database setup and embedding models initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d23cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load documents from the absolute directory: /mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/pv_chatbot_general/design\n",
      "Current working directory of the kernel: /mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/int_hr_chatbot\n",
      "Loading /mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/pv_chatbot_general/design/Chatbot HL_ Appointment Management & Information Dissemination.pdf...\n",
      "Loading /mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/pv_chatbot_general/design/Chatbot Requirements_ Appointment Management & Information Dissemination.pdf...\n",
      "Loaded 39 raw documents.\n",
      "Example source from first loaded document: /mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/pv_chatbot_general/design/Chatbot HL_ Appointment Management & Information Dissemination.pdf\n",
      "Splitting documents into chunks (size=500, overlap=50)...\n",
      "Created 269 chunks.\n",
      "Example source from first chunk: /mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/pv_chatbot_general/design/Chatbot HL_ Appointment Management & Information Dissemination.pdf\n",
      "\n",
      "Sample Chunk 1:\n",
      "High-Level  Design  Document:  Chatbot  \n",
      "Assistant\n",
      " \n",
      "for\n",
      " \n",
      "Healthcare\n",
      " 1.  Introduction  1.1  Background  Vibe  Coding  represents  a  paradigm  shift  in  software  development,  leveraging  Artifici...\n",
      "\n",
      "Sample Chunk 2:\n",
      "interactions\n",
      " \n",
      "for\n",
      " \n",
      "appointment\n",
      " \n",
      "management,\n",
      " \n",
      "pre-admission\n",
      " \n",
      "information,\n",
      " \n",
      "post-discharge\n",
      " \n",
      "instructions,\n",
      " \n",
      "and\n",
      " \n",
      "general\n",
      " \n",
      "information\n",
      " \n",
      "dissemination\n",
      "2.  The  system  aims  to  enhance  patient...\n"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "# Cell 4: Document Loading and Chunking Experimentation\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import glob # To find documents\n",
    "\n",
    "# --- Configuration for Experimentation ---\n",
    "DOCUMENT_PATH = \"../pv_chatbot_general/design/\" # Make sure this directory exists and contains PDFs. Can be relative.\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "# ---------------------------------------\n",
    "# --- Get and Print the Absolute Path ---\n",
    "# Get the absolute path based on the kernel's current working directory\n",
    "# This 'document_path_absolute_dir' should be used for consistency\n",
    "document_path_absolute_dir = os.path.abspath(DOCUMENT_PATH) # This is the directory\n",
    "print(f\"Attempting to load documents from the absolute directory: {document_path_absolute_dir}\")\n",
    "print(f\"Current working directory of the kernel: {os.getcwd()}\")\n",
    "# -----------------------------------------\n",
    "\n",
    "\n",
    "# Load documents\n",
    "documents = []\n",
    "# Use the absolute directory path for glob\n",
    "for rel_file_path_in_glob in glob.glob(os.path.join(document_path_absolute_dir, \"*.pdf\")):\n",
    "    # rel_file_path_in_glob is already absolute because document_path_absolute_dir is absolute\n",
    "    abs_file_path = os.path.abspath(rel_file_path_in_glob) # Ensure it's truly absolute and normalized\n",
    "    print(f\"Loading {abs_file_path}...\")\n",
    "    loader = PyPDFLoader(abs_file_path) # Load using absolute path\n",
    "    loaded_docs_for_file = loader.load()\n",
    "    # Ensure metadata['source'] is the absolute path for all loaded documents\n",
    "    for doc in loaded_docs_for_file:\n",
    "        doc.metadata['source'] = abs_file_path # Explicitly set to ensure consistency\n",
    "    documents.extend(loaded_docs_for_file)\n",
    "\n",
    "print(f\"Loaded {len(documents)} raw documents.\")\n",
    "if documents:\n",
    "    print(f\"Example source from first loaded document: {documents[0].metadata.get('source')}\")\n",
    "\n",
    "# Text Splitter experimentation\n",
    "# Try different chunk sizes and overlaps\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "print(f\"Splitting documents into chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})...\")\n",
    "# The 'chunks' variable will be used in Cell 5\n",
    "# Each chunk will inherit metadata['source'] from its parent document\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks.\")\n",
    "\n",
    "if chunks:\n",
    "    print(f\"Example source from first chunk: {chunks[0].metadata.get('source')}\")\n",
    "    # Inspect some chunks\n",
    "    print(\"\\nSample Chunk 1:\")\n",
    "    print(chunks[0].page_content[:200] + \"...\")\n",
    "    if len(chunks) > 1:\n",
    "        print(\"\\nSample Chunk 2:\")\n",
    "        print(chunks[1].page_content[:200] + \"...\")\n",
    "else:\n",
    "    print(\"No documents were loaded or no chunks were created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1391cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Document Synchronization with Vector DB (PGVector) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5048/3831994370.py:13: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details about the new implementation.\n",
      "  vector_store = PGVector(\n",
      "/tmp/ipykernel_5048/3831994370.py:13: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  vector_store = PGVector(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGVector store initialized for collection: 'internal_knowledge'\n",
      "Using embedding model: HuggingFaceEmbeddings\n",
      "Found 2 files in '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/pv_chatbot_general/design'.\n",
      "--- DIAGNOSTICS --- PRE-ENGINE ACCESS ---\n",
      "Type of vector_store: <class 'langchain_community.vectorstores.pgvector.PGVector'>\n",
      "Is vector_store an instance of PGVector? True\n",
      "Attributes of vector_store (dir(vector_store)):\n",
      "['CollectionStore', 'EmbeddingStore', '__abstractmethods__', '__annotations__', '__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_asimilarity_search_with_relevance_scores', '_bind', '_cosine_relevance_score_fn', '_create_engine', '_create_filter_clause', '_create_filter_clause_deprecated', '_create_filter_clause_json_deprecated', '_distance_strategy', '_embedding_length', '_euclidean_relevance_score_fn', '_from', '_get_retriever_tags', '_handle_field_filter', '_make_session', '_max_inner_product_relevance_score_fn', '_query_collection', '_results_to_docs_and_scores', '_select_relevance_score_fn', '_similarity_search_with_relevance_scores', 'aadd_documents', 'aadd_texts', 'add_documents', 'add_embeddings', 'add_texts', 'adelete', 'afrom_documents', 'afrom_texts', 'aget_by_ids', 'amax_marginal_relevance_search', 'amax_marginal_relevance_search_by_vector', 'as_retriever', 'asearch', 'asimilarity_search', 'asimilarity_search_by_vector', 'asimilarity_search_with_relevance_scores', 'asimilarity_search_with_score', 'collection_metadata', 'collection_name', 'connection_string', 'connection_string_from_db_params', 'create_collection', 'create_extension', 'create_tables_if_not_exists', 'create_vector_extension', 'delete', 'delete_collection', 'distance_strategy', 'drop_tables', 'embedding_function', 'embeddings', 'engine_args', 'from_documents', 'from_embeddings', 'from_existing_index', 'from_texts', 'get_by_ids', 'get_collection', 'get_connection_string', 'logger', 'max_marginal_relevance_search', 'max_marginal_relevance_search_by_vector', 'max_marginal_relevance_search_with_score', 'max_marginal_relevance_search_with_score_by_vector', 'override_relevance_score_fn', 'pre_delete_collection', 'search', 'similarity_search', 'similarity_search_by_vector', 'similarity_search_with_relevance_scores', 'similarity_search_with_score', 'similarity_search_with_score_by_vector', 'use_jsonb']\n",
      "vector_store DOES NOT HAVE 'engine' attribute/property.\n",
      "vector_store MISSING internal attribute: _engine\n",
      "vector_store MISSING internal attribute: _connection_string\n",
      "vector_store MISSING internal attribute: S\n",
      "vector_store has internal attribute: engine_args\n",
      "--- END DIAGNOSTICS ---\n",
      "Found 16 unique document sources in PGVector collection 'internal_knowledge'.\n",
      "Found 16 sources to remove from PGVector: {'/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Content Framework HDI Product Knowledge.pdf', '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/int_hr_chatbot/c:\\\\Users\\\\Tan Prawibowo\\\\Private_PJT\\\\LLM_RAG_WA\\\\LLM_RAG_WA\\\\documents\\\\Tutorial setup AI coding assistant in vscode (3).pdf', '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/int_hr_chatbot/c:\\\\Users\\\\Tan Prawibowo\\\\Private_PJT\\\\LLM_RAG_WA\\\\LLM_RAG_WA\\\\documents\\\\Kubernetes in Action.pdf', '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/int_hr_chatbot/LLM_RAG_WA/documents/AI Develoment 20250518.pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Tutorial setup AI coding assistant in vscode (3).pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Produk dan F&Q HDIndonesia.pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Clean.Code.A.Handbook.of.Agile.Software.Craftsmanship.pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Kubernetes in Action.pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/HDI-in-5-bahasa (hdindonesia).pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/AI Develoment 20250518.pdf', '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/int_hr_chatbot/c:\\\\Users\\\\Tan Prawibowo\\\\Private_PJT\\\\LLM_RAG_WA\\\\LLM_RAG_WA\\\\documents\\\\AI Develoment 20250518.pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Design Patterns Elements of Reusable Object-Oriented Software.pdf', '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/int_hr_chatbot/c:\\\\Users\\\\Tan Prawibowo\\\\Private_PJT\\\\LLM_RAG_WA\\\\LLM_RAG_WA\\\\documents\\\\Clean.Code.A.Handbook.of.Agile.Software.Craftsmanship.pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Produk dan F&Q HDIndonesia [FAQ Only].pdf', '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/int_hr_chatbot/c:\\\\Users\\\\Tan Prawibowo\\\\Private_PJT\\\\LLM_RAG_WA\\\\LLM_RAG_WA\\\\documents\\\\Design Patterns Elements of Reusable Object-Oriented Software.pdf', '/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Produk dan F&Q HDIndonesia [Produk Only].pdf'}\n",
      "Attempting to delete 16060 embedding entries from PGVector for removed files.\n",
      "Deleted embeddings for 16 sources.\n",
      "Preparing to add/update 2 documents. Deleting their old embeddings if they exist...\n",
      "No old embeddings to delete for files being updated/re-added (either new files or no old entries found).\n",
      "Adding 269 chunks (from current documents on disk) to PGVector...\n",
      "Indexing of current documents complete.\n",
      "--- Document Synchronization with Vector DB (PGVector) Complete ---\n"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "# Cell 5: Embedding Generation and Indexing Experimentation\n",
    "from sqlalchemy import text # For executing raw SQL safely\n",
    "\n",
    "print(\"--- Starting Document Synchronization with Vector DB (PGVector) ---\")\n",
    "\n",
    "# Chunks are loaded and processed in Cell 4. Assume 'chunks' variable is available.\n",
    "# Assume 'DOCUMENT_PATH', 'CONNECTION_STRING', 'COLLECTION_NAME', 'embeddings' are available from previous cells.\n",
    "\n",
    "# Initialize PGVector store first to interact with its collections and embeddings\n",
    "# This also ensures its tables (langchain_pg_collection, langchain_pg_embedding) are created if not exist\n",
    "current_embeddings = embeddings # Defaulting to Sentence Transformers as per initial plan\n",
    "vector_store = PGVector(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=current_embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    # pre_delete_collection=False # We want granular control\n",
    ")\n",
    "print(f\"PGVector store initialized for collection: '{COLLECTION_NAME}'\")\n",
    "print(f\"Using embedding model: {current_embeddings.__class__.__name__}\")\n",
    "\n",
    "# Helper function to get Langchain UUIDs for documents with a specific source from PGVector's tables\n",
    "def get_langchain_ids_for_source_from_pgvector(source_path: str, collection_name: str, pg_vector_store_instance: PGVector) -> list[str]:\n",
    "    ids_to_delete = []\n",
    "    with pg_vector_store_instance._bind.connect() as connection: # Changed .engine to ._bind\n",
    "        collection_query = text(\"SELECT uuid FROM langchain_pg_collection WHERE name = :coll_name\")\n",
    "        result = connection.execute(collection_query, {\"coll_name\": collection_name}).fetchone()\n",
    "        if not result:\n",
    "            return [] # Collection doesn't exist, so no IDs to find\n",
    "        collection_id = str(result[0])\n",
    "\n",
    "        embedding_query = text(\n",
    "            \"SELECT uuid FROM langchain_pg_embedding \"\n",
    "            \"WHERE collection_id = :coll_id AND cmetadata->>'source' = :src_path\"\n",
    "        )\n",
    "        embeddings_result = connection.execute(\n",
    "            embedding_query, {\"coll_id\": collection_id, \"src_path\": source_path}\n",
    "        ).fetchall()\n",
    "        ids_to_delete = [str(row[0]) for row in embeddings_result]\n",
    "    return ids_to_delete\n",
    "\n",
    "# 1. Get current file paths from the document directory (ensure absolute paths)\n",
    "current_doc_dir_path = os.path.abspath(DOCUMENT_PATH) # DOCUMENT_PATH from Cell 2, made absolute in Cell 4\n",
    "current_file_paths_on_disk = set()\n",
    "for file_path_glob in glob.glob(os.path.join(current_doc_dir_path, \"*.pdf\")):\n",
    "    current_file_paths_on_disk.add(os.path.abspath(file_path_glob))\n",
    "print(f\"Found {len(current_file_paths_on_disk)} files in '{current_doc_dir_path}'.\")\n",
    "\n",
    "# 2. Get all unique sources currently in the PGVector collection\n",
    "existing_db_sources = set()\n",
    "\n",
    "print(\"--- DIAGNOSTICS --- PRE-ENGINE ACCESS ---\")\n",
    "if 'vector_store' in locals() and vector_store is not None:\n",
    "    print(f\"Type of vector_store: {type(vector_store)}\")\n",
    "    print(f\"Is vector_store an instance of PGVector? {isinstance(vector_store, PGVector)}\")\n",
    "    print(\"Attributes of vector_store (dir(vector_store)):\")\n",
    "    print(dir(vector_store))\n",
    "    if hasattr(vector_store, 'engine'):\n",
    "        print(\"vector_store HAS 'engine' attribute/property.\")\n",
    "        try:\n",
    "            print(f\"Type of vector_store.engine: {type(vector_store.engine)}\")\n",
    "        except Exception as e_engine_access:\n",
    "            print(f\"Error accessing vector_store.engine for type check: {e_engine_access}\")\n",
    "    else:\n",
    "        print(\"vector_store DOES NOT HAVE 'engine' attribute/property.\")\n",
    "    # Check for internal attributes that the 'engine' property might use\n",
    "    for attr_name in ['_engine', '_connection_string', 'S', 'engine_args']:\n",
    "        if hasattr(vector_store, attr_name):\n",
    "            print(f\"vector_store has internal attribute: {attr_name}\")\n",
    "        else:\n",
    "            print(f\"vector_store MISSING internal attribute: {attr_name}\")\n",
    "else:\n",
    "    print(\"vector_store is not defined or is None.\")\n",
    "print(\"--- END DIAGNOSTICS ---\")\n",
    "\n",
    "with vector_store._bind.connect() as conn: # Changed .engine to ._bind\n",
    "    collection_uuid_result = conn.execute(text(\"SELECT uuid FROM langchain_pg_collection WHERE name = :name\"), {\"name\": COLLECTION_NAME}).fetchone()\n",
    "    if collection_uuid_result:\n",
    "        collection_uuid = str(collection_uuid_result[0])\n",
    "        source_query = text(\n",
    "            \"SELECT DISTINCT cmetadata->>'source' as src FROM langchain_pg_embedding WHERE collection_id = :coll_id AND cmetadata->>'source' IS NOT NULL\"\n",
    "        )\n",
    "        results = conn.execute(source_query, {\"coll_id\": collection_uuid}).fetchall()\n",
    "        for res in results:\n",
    "            if res[0]: # Ensure source is not None\n",
    "                 # Paths stored in cmetadata should already be absolute if Cell 4 is correct\n",
    "                existing_db_sources.add(os.path.abspath(res[0]))\n",
    "print(f\"Found {len(existing_db_sources)} unique document sources in PGVector collection '{COLLECTION_NAME}'.\")\n",
    "\n",
    "# 3. Identify and delete embeddings for documents no longer on disk\n",
    "files_to_remove_sources = existing_db_sources - current_file_paths_on_disk\n",
    "ids_for_final_deletion = []\n",
    "if files_to_remove_sources:\n",
    "    print(f\"Found {len(files_to_remove_sources)} sources to remove from PGVector: {files_to_remove_sources}\")\n",
    "    for source_to_remove in files_to_remove_sources:\n",
    "        ids = get_langchain_ids_for_source_from_pgvector(source_to_remove, COLLECTION_NAME, vector_store)\n",
    "        if ids:\n",
    "            ids_for_final_deletion.extend(ids)\n",
    "    if ids_for_final_deletion:\n",
    "        print(f\"Attempting to delete {len(ids_for_final_deletion)} embedding entries from PGVector for removed files.\")\n",
    "        vector_store.delete(ids=ids_for_final_deletion)\n",
    "        print(f\"Deleted embeddings for {len(files_to_remove_sources)} sources.\")\n",
    "    else:\n",
    "        print(\"No specific embedding entries found for deletion for removed files.\")\n",
    "else:\n",
    "    print(\"No documents (sources) to remove from PGVector.\")\n",
    "\n",
    "# 4. Delete old embeddings for files that will be updated/re-added\n",
    "# These are all files currently on disk. We delete their old versions before adding new ones.\n",
    "sources_to_re_add_or_update = current_file_paths_on_disk\n",
    "ids_for_pre_update_deletion = []\n",
    "if sources_to_re_add_or_update:\n",
    "    print(f\"Preparing to add/update {len(sources_to_re_add_or_update)} documents. Deleting their old embeddings if they exist...\")\n",
    "    for source_path in sources_to_re_add_or_update:\n",
    "        # Only try to delete if this source was previously known to be in the DB (optimization)\n",
    "        if source_path in existing_db_sources:\n",
    "            ids = get_langchain_ids_for_source_from_pgvector(source_path, COLLECTION_NAME, vector_store)\n",
    "            if ids:\n",
    "                ids_for_pre_update_deletion.extend(ids)\n",
    "    \n",
    "    if ids_for_pre_update_deletion:\n",
    "        print(f\"Attempting to delete {len(ids_for_pre_update_deletion)} old embedding entries for files being updated/re-added.\")\n",
    "        vector_store.delete(ids=ids_for_pre_update_deletion)\n",
    "        print(f\"Deleted old embeddings for files being updated/re-added.\")\n",
    "    else:\n",
    "        print(\"No old embeddings to delete for files being updated/re-added (either new files or no old entries found).\")\n",
    "else:\n",
    "    print(\"No current files on disk to process for add/update.\")\n",
    "\n",
    "# 5. Add new/updated chunks to the vector store\n",
    "# 'chunks' variable comes from Cell 4. Each chunk has metadata['source'] as an absolute path.\n",
    "if 'chunks' in locals() and chunks:\n",
    "    valid_chunks = [chunk for chunk in chunks if chunk.metadata.get(\"source\")]\n",
    "    if not valid_chunks and chunks:\n",
    "        print(\"Warning: Some chunks from Cell 4 are missing source metadata. They will be skipped.\")\n",
    "    \n",
    "    if valid_chunks:\n",
    "        print(f\"Adding {len(valid_chunks)} chunks (from current documents on disk) to PGVector...\")\n",
    "        vector_store.add_documents(valid_chunks)\n",
    "        print(\"Indexing of current documents complete.\")\n",
    "    else:\n",
    "        print(\"No valid chunks with source metadata to add to PGVector.\")\n",
    "elif 'chunks' in locals() and not chunks:\n",
    "     print(\"No chunks were created from documents in Cell 4.\")\n",
    "else:\n",
    "    print(\"No chunks to add to PGVector ('chunks' variable not defined or not loaded from Cell 4).\")\n",
    "\n",
    "print(\"--- Document Synchronization with Vector DB (PGVector) Complete ---\")\n",
    "\n",
    "# Note: The SQLAlchemy model 'DocumentChunk' and its table 'document_chunks' (defined in Cell 3)\n",
    "# are not directly used by this PGVector setup for RAG. PGVector manages its own schema\n",
    "# (tables like 'langchain_pg_embedding', 'langchain_pg_collection') and the 'COLLECTION_NAME'\n",
    "# refers to a collection within that schema. This solution works with Langchain's default PGVector behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a78f9d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLM_MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading LLM model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mLLM_MODEL_NAME\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# For Ollama (local)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'LLM_MODEL_NAME' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading LLM model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mLLM_MODEL_NAME\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m     llm = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Ensure llm is None if loading failed\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# ... rest of the cell\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'LLM_MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "# Inside Cell 6: Retrieval and RAG Chain Experimentation\n",
    "# ...\n",
    "\n",
    "# Import your chosen LLM\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # <-- Ensure this is imported\n",
    "GOOGLE_API_KEY = 'AIzaSyBsErNvPHekr4z2meNLLek9z1sMUC-TeU8'\n",
    "# ...\n",
    "# Initialize LLM (adjust based on your choice)\n",
    "try:\n",
    "    print(f\"Loading LLM model: {LLM_MODEL_NAME}\")\n",
    "    # For Ollama (local)\n",
    "    if not GOOGLE_API_KEY: # Only use Ollama if no Google Key is available\n",
    "         llm = Ollama(model=LLM_MODEL_NAME)\n",
    "    # For Google Generative AI (uncomment and configure GOOGLE_API_KEY)\n",
    "    if GOOGLE_API_KEY and LLM_MODEL_NAME.startswith(\"gemini\"):\n",
    "       print(f\"Attempting to load Google LLM model: {LLM_MODEL_NAME}\")\n",
    "       # Ensure the API key is passed correctly to the client\n",
    "       llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, google_api_key=GOOGLE_API_KEY) # <-- Pass the key here\n",
    "       print(f\"Google LLM model '{LLM_MODEL_NAME}' loaded.\")\n",
    "    elif not GOOGLE_API_KEY:\n",
    "         # Fallback or error if LLM_MODEL_NAME was a Google model but no key was found\n",
    "         print(f\"GOOGLE_API_KEY not set. Cannot load Google model '{LLM_MODEL_NAME}'. Falling back or failing.\")\n",
    "         # Optional: Add fallback to Ollama or raise an error\n",
    "         llm = Ollama(model=LLM_MODEL_NAME) # Fallback example\n",
    "    else:\n",
    "        print(f\"LLM_MODEL_NAME '{LLM_MODEL_NAME}' is not a Google model or GOOGLE_API_KEY not set. Using Ollama or default.\")\n",
    "        llm = Ollama(model=LLM_MODEL_NAME) # Defaulting to Ollama if conditions not met\n",
    "\n",
    "    # If both attempts failed\n",
    "    if 'llm' not in locals() or llm is None:\n",
    "         raise ValueError(f\"Could not initialize LLM model '{LLM_MODEL_NAME}' with available credentials.\")\n",
    "\n",
    "    print(\"LLM loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model '{LLM_MODEL_NAME}': {e}\")\n",
    "    llm = None # Ensure llm is None if loading failed\n",
    "\n",
    "# ... rest of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f6aa155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model: gemini-2.5-flash\n",
      "LLM loaded.\n",
      "\n",
      "Running query: 'what is the app about ?'\n",
      "\n",
      "--- Response ---\n",
      "The app is related to a custom hospital app UI for Appointment Management & Information Dissemination.\n",
      "Source: Document(metadata={'title': 'Chatbot HL: Appointment Management & Information Dissemination', 'page': 5})\n"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "# Cell 6: Retrieval and RAG Chain Experimentation\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Import your chosen LLM\n",
    "#from langchain_community.llms import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Uncomment if using Google LLM\n",
    "\n",
    "# --- Configuration for Experimentation ---\n",
    "# LLM Model (e.g., \"llama2\" for Ollama, \"gemini-pro\" for Google)\n",
    "#LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\", \"llama2\") # Adjust based on your setup\n",
    "LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\", \"gemini-2.5-flash\") # Adjust based on your setup\n",
    "\n",
    "RETRIEVAL_K = 4 # Number of chunks to retrieve\n",
    "SEARCH_TYPE = \"similarity\" # or \"mmr\" (Maximal Marginal Relevance)\n",
    "# ---------------------------------------\n",
    "\n",
    "# Initialize LLM (adjust based on your choice)\n",
    "try:\n",
    "    print(f\"Loading LLM model: {LLM_MODEL_NAME}\")\n",
    "    # For Ollama\n",
    "    #llm = Ollama(model=LLM_MODEL_NAME)\n",
    "    # For Google Generative AI (uncomment and configure GOOGLE_API_KEY)\n",
    "    # if GOOGLE_API_KEY and LLM_MODEL_NAME.startswith(\"gemini\"):\n",
    "    llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, google_api_key=GOOGLE_API_KEY)\n",
    "    # else:\n",
    "    #    llm = Ollama(model=LLM_MODEL_NAME) # Fallback or different local LLM\n",
    "    print(\"LLM loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model '{LLM_MODEL_NAME}': {e}\")\n",
    "    llm = None\n",
    "\n",
    "\n",
    "if vector_store and llm:\n",
    "    # Configure the retriever\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": RETRIEVAL_K}, search_type=SEARCH_TYPE)\n",
    "\n",
    "    # Define the RAG Prompt Template (incorporating ideas from the Prompt Engineering PDF)\n",
    "    # Use a System Prompt for context and persona\n",
    "    # Use Contextual Prompting by including retrieved documents\n",
    "    # Consider Few-shot examples if helpful (add to the prompt)\n",
    "    template = \"\"\"\n",
    "    You are an AI assistant specialized in answering questions based on the provided context.\n",
    "    Answer the user's question truthfully and concisely, using ONLY the information from the following documents.\n",
    "    If the documents do not contain the answer, state that you cannot find the answer in the provided information.\n",
    "    Do NOT make up information.\n",
    "    Cite the source document(s) (if metadata is available) for your answer.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # Build the RAG Chain\n",
    "    # 1. Retrieve documents based on the question\n",
    "    # 2. Format the retrieved documents into a single string for the prompt context\n",
    "    # 3. Pass the context and question to the prompt template\n",
    "    # 4. Send the filled prompt to the LLM\n",
    "    # 5. Parse the LLM's output\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # --- Run a test query ---\n",
    "    query = \"what is the app about ?\" # Replace with a query relevant to your documents\n",
    "\n",
    "    print(f\"\\nRunning query: '{query}'\")\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(\"\\n--- Response ---\")\n",
    "    print(response)\n",
    "\n",
    "    # --- Experimentation Ideas ---\n",
    "    # Run different queries\n",
    "    # Change RETRIEVAL_K or SEARCH_TYPE\n",
    "    # Change the LLM model or its parameters (temperature, etc.)\n",
    "    # Refine the prompt template\n",
    "    # Evaluate the response against your gold standard evaluation set (ideally automated)\n",
    "    # Track metrics like retrieval time, LLM response time\n",
    "else:\n",
    "    print(\"Vector store or LLM not initialized. Cannot run RAG chain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e2f55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "# Cell 7: (Optional) Automated Evaluation Snippet\n",
    "\n",
    "# This is a simplified example. A full evaluation framework might be more complex.\n",
    "# You would load your gold standard questions and ground truth answers here.\n",
    "\n",
    "gold_standard = [\n",
    "    {\"question\": \"What is X?\", \"answer\": \"According to document Y, X is Z.\"},\n",
    "    # Add more questions and ground truth answers\n",
    "]\n",
    "\n",
    "# Iterate through your gold standard questions\n",
    "# For each question:\n",
    "# 1. Run the rag_chain.invoke(question)\n",
    "# 2. Compare the rag_chain's response to the expected answer in your gold standard.\n",
    "#    This comparison is tricky for LLM outputs (semantic similarity, faithfulness).\n",
    "#    You might need evaluation metrics (e.g., RAGAS library, or manual scoring).\n",
    "# 3. Record the results (question, expected answer, actual answer, perhaps a score).\n",
    "\n",
    "# Example (conceptual):\n",
    "# evaluation_results = []\n",
    "# for item in gold_standard:\n",
    "#     question = item[\"question\"]\n",
    "#     expected_answer = item[\"answer\"]\n",
    "#     actual_response = rag_chain.invoke(question)\n",
    "#     # Implement comparison/scoring logic here\n",
    "#     score = score_response(actual_response, expected_answer) # Your custom scoring function\n",
    "#     evaluation_results.append({\"question\": question, \"expected\": expected_answer, \"actual\": actual_response, \"score\": score})\n",
    "\n",
    "# Print/Save evaluation results\n",
    "# print(\"\\n--- Evaluation Results ---\")\n",
    "# for res in evaluation_results:\n",
    "#     print(f\"Q: {res['question']}\\nA: {res['actual']}\\nScore: {res['score']}\\n---\")\n",
    "\n",
    "# Challenging Question: How do you define and automatically measure the 'faithfulness' and 'relevance' of the AI's response against your documents and the user's query? Comparing LLM text output programmatically to a 'ground truth' is hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bc2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /home/tan.joenarto/.local/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tan.joenarto/.local/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "RAG Service Ask Endpoint: http://localhost:8000/v1/ask\n",
      "RAG Service Health Endpoint: http://localhost:8000/v1/health\n"
     ]
    }
   ],
   "source": [
    "#Test with\n",
    "%pip install requests \n",
    "# Or poetry add requests if managing notebook dependencies with poetry\n",
    "# Cell 1: Import Libraries\n",
    "\n",
    "import requests\n",
    "import json # Useful for pretty printing JSON\n",
    "\n",
    "# --- Configuration ---\n",
    "# The URL where your local rag-service is running\n",
    "# Ensure your uvicorn command includes --host 0.0.0.0\n",
    "RAG_SERVICE_URL = \"http://localhost:8000\"\n",
    "\n",
    "# The specific endpoint for asking questions\n",
    "ASK_ENDPOINT = f\"{RAG_SERVICE_URL}/v1/ask\"\n",
    "\n",
    "# The specific endpoint for health check\n",
    "HEALTH_ENDPOINT = f\"{RAG_SERVICE_URL}/v1/health\"\n",
    "# -------------------\n",
    "\n",
    "print(f\"RAG Service Ask Endpoint: {ASK_ENDPOINT}\")\n",
    "print(f\"RAG Service Health Endpoint: {HEALTH_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9a7f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Health Endpoint: http://localhost:8000/v1/health\n",
      "Status Code: 200\n",
      "Health Check Successful!\n",
      "Response Body:\n",
      "{\n",
      "  \"status\": \"ok\",\n",
      "  \"message\": \"RAG service is healthy.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test the Health Endpoint\n",
    "\n",
    "print(f\"Testing Health Endpoint: {HEALTH_ENDPOINT}\")\n",
    "\n",
    "try:\n",
    "    # Send GET request to the health endpoint\n",
    "    response = requests.get(HEALTH_ENDPOINT)\n",
    "\n",
    "    # Check the response status code\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "    # Check if the request was successful (status code 200-299)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Health Check Successful!\")\n",
    "        # Parse and print the JSON response body\n",
    "        print(\"Response Body:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(f\"Health Check Failed. Response:\")\n",
    "        print(response.text) # Print response text if not successful\n",
    "\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"Error: Could not connect to the RAG service at {HEALTH_ENDPOINT}.\")\n",
    "    print(\"Please ensure the service is running locally.\")\n",
    "    print(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during health check: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05979933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Ask Endpoint: http://localhost:8000/v1/ask\n",
      "Sending question: 'what the beenfit of the app about?'\n",
      "Status Code: 200\n",
      "Request Successful!\n",
      "\n",
      "--- Answer ---\n",
      "Answer Text: The benefits of the app include:\n",
      "*   It can be downloaded and executed with a single command without requiring installation, even for complex applications with many dependencies (27Creating, running, and sharing a container image).\n",
      "*   Developers involved in running the application in production gain a better understanding of user needs and issues, as well as problems faced by the operations team. This also encourages developers to release the app earlier and use user feedback to guide further development (7Introducing container technologies).\n",
      "Sources: ['/mnt/c/Users/Tan Prawibowo/Private_PJT/LLM_RAG_WA/LLM_RAG_WA/documents/Kubernetes in Action.pdf', 'c:\\\\Users\\\\Tan Prawibowo\\\\Private_PJT\\\\LLM_RAG_WA\\\\LLM_RAG_WA\\\\documents\\\\Kubernetes in Action.pdf', '/mnt/c/Users/Tan Prawibowo/BTH_AWS_TF/pv_chatbot_general/design/Chatbot HL_ Appointment Management & Information Dissemination.pdf']\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test the Ask Endpoint\n",
    "\n",
    "print(f\"\\nTesting Ask Endpoint: {ASK_ENDPOINT}\")\n",
    "\n",
    "# Define the request payload (matching your Question Pydantic model)\n",
    "question_payload = {\n",
    "    \"text\": \"what the beenfit of the app about?\" # Replace with a question from your gold standard evaluation set or documents\n",
    "}\n",
    "\n",
    "print(f\"Sending question: '{question_payload['text']}'\")\n",
    "\n",
    "try:\n",
    "    # Send POST request to the ask endpoint\n",
    "    response = requests.post(\n",
    "        ASK_ENDPOINT,\n",
    "        json=question_payload, # 'json' parameter automatically sets Content-Type to application/json and serializes the dict\n",
    "        # headers={'Content-Type': 'application/json'} # 'json' param handles this\n",
    "    )\n",
    "\n",
    "    # Check the response status code\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Request Successful!\")\n",
    "        # Parse and print the JSON response body (matching your Answer Pydantic model)\n",
    "        answer_data = response.json()\n",
    "        print(\"\\n--- Answer ---\")\n",
    "        print(f\"Answer Text: {answer_data.get('text', 'N/A')}\") # Use .get to avoid KeyError if fields are missing\n",
    "        print(f\"Sources: {answer_data.get('sources', [])}\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Request Failed. Status Code: {response.status_code}\")\n",
    "        print(\"Response Body:\")\n",
    "        try:\n",
    "            print(json.dumps(response.json(), indent=2)) # Try to print as JSON if possible\n",
    "        except json.JSONDecodeError:\n",
    "            print(response.text) # Otherwise, print as plain text\n",
    "\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"Error: Could not connect to the RAG service at {ASK_ENDPOINT}.\")\n",
    "    print(\"Please ensure the service is running locally.\")\n",
    "    print(f\"Details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during API call: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
